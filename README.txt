# Dyna-CPPO: Model-Based Constrained Policy Optimization

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python](https://img.shields.io/badge/Python-3.10-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)

**Dyna-CPPO** is the official implementation of a Safe Reinforcement Learning algorithm that combines **Dyna-style model-based planning** with **Constrained Policy Optimization (CPPO)**.

The algorithm leverages a learned `DynamicsModel` to generate "imaginary rollouts" (predicted trajectories of state, reward, and cost). By combining these rollouts with a **Dual-Critic** architecture and an **Expectation-Maximization (EM)** update rule, Dyna-CPPO achieves high sample efficiency while strictly adhering to safety constraints.

## ðŸ“‚ Project Structure

The repository structure is organized as follows:

```text
dyna-cppo/
â”œâ”€â”€ agents/                 # Neural Network Modules
â”‚   â”œâ”€â”€ policy.py           # Actor: Gaussian Policy Network
â”‚   â”œâ”€â”€ value.py            # Critics: ValueNet (Reward) & CostNet (Cost)
â”‚   â””â”€â”€ dynamics.py         # Model: Dynamics Model (s,a -> s',r,c)
â”œâ”€â”€ algo/                   # CPPO Core Algorithm
â”‚   â”œâ”€â”€ advantages.py       # Dual GAE: Reward Advantage (A_r) & Cost Advantage (A_c)
â”‚   â”œâ”€â”€ estep.py            # E-Step: Analytical solution for optimal policy ratio v*
â”‚   â””â”€â”€ mstep.py            # M-Step: Supervised policy update via MSE Loss
â”œâ”€â”€ envs/                   # Environment Interfaces
â”‚   â”œâ”€â”€ safety_gym_wrappers.py # Factory for Safety-Gymnasium environments
â”‚   â””â”€â”€ circle_wrappers.py     # Custom Circle task wrappers
â”œâ”€â”€ utils/                  # Utilities
â”‚   â”œâ”€â”€ log.py              # Logger: Training monitoring and file saving
â”‚   â”œâ”€â”€ seed.py             # Seed: Global random seed setting for reproducibility
â”‚   â”œâ”€â”€ schedule.py         # Schedule: Linear annealing for Learning Rate and Clip Range
â”‚   â””â”€â”€ replay_buffer.py    # Buffer: Experience Replay Buffer
â”œâ”€â”€ logs/                   # Experiment Outputs
â”‚   â”œâ”€â”€ *.txt               # Detailed training logs (e.g., cppo_goal1.txt)
â”‚   â”œâ”€â”€ *.csv               # Plotting data (e.g., cppo_goals_reward.csv)
â”‚   â””â”€â”€ *.pt                # Model checkpoints (e.g., policy_final_goal0.pt)
â”œâ”€â”€ main.py                 # Main Entry Point
â”œâ”€â”€ environment.yml         # Dependency Manifest
â””â”€â”€ README.md               # Documentation

